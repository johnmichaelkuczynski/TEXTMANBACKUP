# NEUROTEXT: COMPLETE SYSTEM SPECIFICATION FOR REPLIT AGENT

---

## PART 1: DATABASE AND LENGTH REQUIREMENTS

I need to build a database that will make it possible for the app to process and generate documents of ANY length — from 1 word to 500,000 words.

### Length Requirements (NON-NEGOTIABLE)

- If the user asks for 90,000 words, they get **at least 90,000 words**. Not 78,000. Not 89,000. Not 89,999.
- The output length must **ALWAYS EQUAL OR EXCEED** what the user asks for.
- This applies to ALL lengths. No exceptions.
- If the system cannot hit the target, it must **keep generating** until it does — or fail explicitly and tell the user why.

### Coherence Requirements (NON-NEGOTIABLE)

- All documents must be **100% coherent** regardless of length.
- This requires the database-enforced multi-pass pipeline:
  - **Pass 1**: Extract and store global skeleton (thesis, outline, key terms, commitments)
  - **Pass 2**: Process each chunk (~1000-1400 words) with mandatory DB reads/writes:
    - READ skeleton from DB before each chunk
    - READ all prior deltas from DB before each chunk
    - WRITE chunk output to DB after each chunk
    - WRITE chunk delta to DB after each chunk
    - 15-second pause between chunks
  - **Pass 3**: Global stitch pass comparing all deltas, detecting conflicts, executing repairs
- **If the database is not being used, coherence is impossible at scale. Period.**

### Storage Requirements (NON-NEGOTIABLE)

- The database must store **absolutely everything** generated by the user.
- **Permanently.** Until I manually delete it.
- This includes:
  - Every input document
  - Every output document
  - Every skeleton extracted
  - Every chunk (input, output, delta)
  - Every stitch result
  - Every error
  - Every audit log (see Part 2)
- Nothing is temporary. Nothing is in-memory only. Everything persists.

---

## PART 2: MANDATORY AUDIT SYSTEM

**NeuroText is not allowed to silently "process" and then deposit a finished output.**

If it does, it will cheat: skip database writes, run in-memory only, truncate output, ignore the coherence pipeline, or claim it used the DB when it didn't.

### Every Operation Must Produce TWO Outputs

**OUTPUT 1**: The user's requested output (the document, rewrite, analysis, etc.)

**OUTPUT 2**: A permanent, streaming audit log documenting **every single action** the system took

### Audit Log: Real-Time Streaming

Whenever the user triggers ANY operation:

1. **Immediately open a live Audit Panel** (popup/side-panel)
2. **Stream every action in real time**, line by line, as it happens
3. **Nothing happens invisibly** — if it's not in the log, it didn't happen

The stream must show:

- Every database query (actual SQL text)
- Every table accessed (which table, read or write)
- Every row inserted (table name, row ID, key fields)
- Every row updated (table name, row ID, what changed)
- Every LLM call:
  - Model name
  - Token count (input/output)
  - Prompt summary
  - Response summary
  - Latency (ms)
- Every chunk processed:
  - Chunk index
  - Input word count
  - Output word count
  - Target word count
  - Pass/fail
- Every skeleton extraction
- Every delta recorded
- Every stitch operation
- Every repair attempt
- Every error (full error text)
- Timestamps for every action

### Audit Log: User Access

The user must be able to:

1. **Watch it live** as it streams
2. **Scroll back** through the log during generation
3. **Copy** any portion
4. **Download** the full log (JSON or TXT)
5. **Access it after completion** — stays visible until dismissed
6. **Retrieve it later** from their account (Job History page)

### Audit Log: Permanent Storage

Every audit log must be:

1. **Stored in the database immediately** as events occur
2. **Linked to the user's account**
3. **Linked to the output it produced**
4. **Accessible from a Job History / Audit Logs section**
5. **Never auto-deleted**

---

## PART 3: DATABASE SCHEMA REQUIREMENTS

### Core Tables (must exist and be actively used)

```sql
-- User sessions for coherence pipeline
CREATE TABLE coherent_sessions (
  id SERIAL PRIMARY KEY,
  user_id INTEGER REFERENCES users(id) NOT NULL,
  session_type TEXT NOT NULL,
  user_prompt TEXT NOT NULL,
  status TEXT NOT NULL DEFAULT 'pending',
  global_skeleton JSONB,
  total_chunks INTEGER DEFAULT 0,
  processed_chunks INTEGER DEFAULT 0,
  target_words INTEGER,
  actual_words INTEGER,
  created_at TIMESTAMP NOT NULL DEFAULT NOW(),
  updated_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Chunks with full tracking
CREATE TABLE coherent_chunks (
  id SERIAL PRIMARY KEY,
  session_id INTEGER REFERENCES coherent_sessions(id) ON DELETE CASCADE NOT NULL,
  chunk_index INTEGER NOT NULL,
  chunk_type TEXT NOT NULL,
  chunk_input TEXT,
  chunk_output TEXT,
  chunk_delta JSONB,
  target_words INTEGER,
  actual_words INTEGER,
  processed_at TIMESTAMP,
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Stitch results
CREATE TABLE stitch_results (
  id SERIAL PRIMARY KEY,
  session_id INTEGER REFERENCES coherent_sessions(id) ON DELETE CASCADE NOT NULL,
  conflicts JSONB,
  repairs JSONB,
  final_validation JSONB,
  coherence_score TEXT,
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Audit logs (parent)
CREATE TABLE audit_logs (
  id SERIAL PRIMARY KEY,
  user_id INTEGER REFERENCES users(id) NOT NULL,
  job_type TEXT NOT NULL,
  job_id INTEGER,
  started_at TIMESTAMP NOT NULL DEFAULT NOW(),
  completed_at TIMESTAMP,
  status TEXT DEFAULT 'running',
  final_output_preview TEXT,
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Audit log entries (streaming events)
CREATE TABLE audit_log_entries (
  id SERIAL PRIMARY KEY,
  audit_log_id INTEGER REFERENCES audit_logs(id) NOT NULL,
  sequence_num INTEGER NOT NULL,
  timestamp TIMESTAMP NOT NULL DEFAULT NOW(),
  event_type TEXT NOT NULL,
  event_data JSONB NOT NULL,
  created_at TIMESTAMP NOT NULL DEFAULT NOW()
);

-- Indexes
CREATE INDEX idx_coherent_chunks_session ON coherent_chunks(session_id);
CREATE INDEX idx_stitch_session ON stitch_results(session_id);
CREATE INDEX idx_audit_logs_user ON audit_logs(user_id);
CREATE INDEX idx_audit_entries_log ON audit_log_entries(audit_log_id);
```

---

## PART 4: VERIFICATION REQUIREMENTS

### How to Prove the System Works

After ANY generation, the following must be verifiable:

1. **Row counts increased** in coherent_sessions, coherent_chunks, stitch_results
2. **Audit log exists** with entries for every DB operation and LLM call
3. **Output word count** equals or exceeds target
4. **Timestamps on chunks** are ~15 seconds apart (proving sequential processing)
5. **Skeleton stored** in coherent_sessions.global_skeleton
6. **Deltas stored** in coherent_chunks.chunk_delta for every chunk

### How to Prove the System is Broken

If ANY of these are true, the system is broken:

- Tables show 0 rows after a generation
- Audit log is empty or missing
- Output word count is below target
- Chunks have no timestamps or identical timestamps
- Skeleton is null
- Deltas are null

---

## PART 5: THE RULE

**If it's not in the audit log, it didn't happen.**
**If it's not in the database, it wasn't saved.**
**If the output is short, the system failed.**

The audit log is not a debugging feature. It is **proof that the system did what it claims**.

The database is not optional. It is **the mechanism that makes coherence possible**.

The length target is not a suggestion. It is **a hard requirement**.

---

**BUILD THIS.**